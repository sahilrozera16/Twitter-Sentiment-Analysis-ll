{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV file\n",
    "train = pd.read_csv ('train_2kmZucJ.csv')\n",
    "test = pd.read_csv ('test_oJQbWVk.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Both Datasets\n",
    "combine = train.append(test,ignore_index=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...\n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...\n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...\n",
       "4   5    1.0  What amazing service! Apple won't even talk to..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>9869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9869</th>\n",
       "      <td>9870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Now Available - Hoodie. Check it out here - ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9870</th>\n",
       "      <td>9871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There goes a crack right across the screen. If...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>9872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@codeofinterest as i said #Adobe big time we m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>9873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finally I got it .. thanx my father .. #Samsun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label                                              tweet\n",
       "9868  9869    NaN  #SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...\n",
       "9869  9870    NaN  Now Available - Hoodie. Check it out here - ht...\n",
       "9870  9871    NaN  There goes a crack right across the screen. If...\n",
       "9871  9872    NaN  @codeofinterest as i said #Adobe big time we m...\n",
       "9872  9873    NaN  Finally I got it .. thanx my father .. #Samsun..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 5 records\n",
    "combine.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9873, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9873 entries, 0 to 9872\n",
      "Data columns (total 3 columns):\n",
      "id       9873 non-null int64\n",
      "label    7920 non-null float64\n",
      "tweet    9873 non-null object\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 231.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# View data information\n",
    "combine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5894\n",
       "1    2026\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count of training dataset\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokening the Data With spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we’re working with, let’s create a custom tokenizer function using `spaCy`. We’ll use this function to automatically strip information we don’t need, like stopwords and punctuation, from each review.\n",
    "\n",
    "We’ll start by importing the English models we need from `spaCy`, as well as Python’s `string` module, which contains a helpful list of all punctuation marks that we can use in `string.punctuation`. We’ll create variables that contain the punctuation marks and stopwords we want to remove, and a parser that runs input through `spaCy‘s` English module.\n",
    "\n",
    "Then, we’ll create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words. This is similar to what we did in the examples earlier in this tutorial, but now we’re putting it all together into a single function for preprocessing each user review we’re analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and converting text into lower case. Here, we will create a custom `predictors` class wich inherits the `TransformerMixin` class. This class overrides the transform, fit and get_parrams methods. We’ll also create a `clean_text()` function that removes spaces and converts text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we classify text, we end up with text snippets matched with their respective labels. But we can’t simply use text strings in our machine learning model; we need a way to convert our text into something that can be represented numerically just like the labels (0 for positive and 1 for negative) are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically.\n",
    "\n",
    "One tool we can use for doing this is called `Bag of Words`. BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.\n",
    "\n",
    "We can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling `CountVectorizer` to use the custom `spacy_tokenizer` function we built as its tokenizer, and defining the ngram range we want.\n",
    "\n",
    "`N-grams` are combinations of adjacent words in a given text, where `n` is the number of words that incuded in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the `ngram_range` parameter we’ll use in the code below sets the lower and upper bounds of the our `ngrams` (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting The Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['tweet'] # the features we want to analyze\n",
    "y = train['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline and Generating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’re all set up, it’s time to actually build our model! We’ll start by importing the LogisticRegression module and creating a `LogisticRegression` classifier object.\n",
    "\n",
    "Then, we’ll create a `pipeline` with three components: a cleaner, a vectorizer, and a classifier. The `cleaner` uses our predictors class object to clean and preprocess the text. The `vectorizer` uses countvector objects to create the bag of words matrix for our text. The `classifier` is an object that performs the logistic regression to classify the sentiments.\n",
    "\n",
    "Once this pipeline is built, we’ll fit the pipeline components using fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x000001EADD053BC8>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      1748\n",
      "           1       0.78      0.82      0.80       628\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2376\n",
      "   macro avg       0.86      0.87      0.86      2376\n",
      "weighted avg       0.89      0.89      0.89      2376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918350168350169"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I hate the new #iphone upgrade. Won't let me d...\n",
       "1       currently shitting my fucking pants. #apple #i...\n",
       "2       I'd like to puts some CD-ROMS on my iPad, is t...\n",
       "3       My ipod is officially dead. I lost all my pict...\n",
       "4       Been fighting iTunes all night! I only want th...\n",
       "5       #Repost @getbakednfried with repostapp ・・・ to ...\n",
       "6       This new apple software update is really doing...\n",
       "7       BABY #iPhone #iphone6s #gold #new #apple #appl...\n",
       "8       I'm confused...why did I have to take the time...\n",
       "9       Fruit just tastes better when you pick it your...\n",
       "10      Con mi buddy#edgar #buddy #friend #viviendo #t...\n",
       "11      #MyMomIsGreatBecause she always care about me!...\n",
       "12      #kiss,,#romance,#hot,#movie,#iphone,#sex,#porn...\n",
       "13      Holding to hope when a loved one is… http://dl...\n",
       "14      New phone case :) #Me #Selfie #Guy #Dude #Smil...\n",
       "15      this #shoot from my #son (10) at #Swarovski #K...\n",
       "16      Found this photo of ME! Not mine, still its ME...\n",
       "17      10 fucking iPhone chargers && none work, is th...\n",
       "18      I can't buy a song off iTunes w/o having to go...\n",
       "19      Check out the Justin Layman #app http://rvrb.f...\n",
       "20      Good morning golden October #me #new #RoseGold...\n",
       "21      Got to ride my horse properly for the first ti...\n",
       "22      Got my iPhone screen fixed..get into bed phone...\n",
       "23      Samsung permanently stops Galaxy Note 7 produc...\n",
       "24      can I just add that right after I tweeted abou...\n",
       "25        #2do one of the best apps for the #iphone it :)\n",
       "26      15% DISCOUNT https://bit.ly/2vUfGuH #zazzle #p...\n",
       "27      Reposting @lexzaronis: “If you want to see the...\n",
       "28      My iPhone 5 charger has broken. I bought two n...\n",
       "29      Tapeless!!!? Just call me crazy! Lol... #jj #s...\n",
       "                              ...                        \n",
       "1923    Photo: I am calm this time. Please no security...\n",
       "1924    Pulsemeter - , #Hate and #Friendship, with Gen...\n",
       "1925    U2's new al$&@*# has infiltrated my playlist ....\n",
       "1926    Shana Tova, happy new #Jewish year from @Secre...\n",
       "1927    RT @engadget: Apple pulls VLC from the iTunes ...\n",
       "1928    Gifts from my wife for her Hubby Aani #MeemAli...\n",
       "1929    Fall in love with your iPhone again this #vale...\n",
       "1930    Aww...went to send a bday text, and found out ...\n",
       "1931    We've released the device that we were suppose...\n",
       "1932    sponsorhip program. Thank You! #christmas #lif...\n",
       "1933    Happy in Zeeland Today ;-) #sunset #zeeland #n...\n",
       "1934    Haven't posted a pic in a while. #girl #iphone...\n",
       "1935    Could this be @Samsung Galaxy10? @letsgodigita...\n",
       "1936    RT @XL_Radio Enjoy devotional #gurbani keertan...\n",
       "1937    listening to #spotify @SpotifyUK #music on my ...\n",
       "1938    Motoception. Welcome to the family, Moto X2 :)...\n",
       "1939    Babay..see youuu :( #iphone5 #iphone #silver #...\n",
       "1940    : ADAM SUTCH OR FREAKING KILL YOURSELF. #Me #D...\n",
       "1941    I my #eyes they are the windows to my soul #in...\n",
       "1942    @krissyannn omgsh boo I just saw your text, I ...\n",
       "1943    My Men at the top of Gummers How. #mommyhood #...\n",
       "1944    I am thoroughly addicted to the #AngryBirds ga...\n",
       "1945    #me #girl #Brazil #life #GalaxyS3 #samsung #Sa...\n",
       "1946    Whoop! Raj of @bigbangtheory is not alone, my ...\n",
       "1947    Are you a camera?because everytime i look at y...\n",
       "1948    #SamsungGalaxyNote7 Explodes, Burns 6-Year-Old...\n",
       "1949    Now Available - Hoodie. Check it out here - ht...\n",
       "1950    There goes a crack right across the screen. If...\n",
       "1951    @codeofinterest as i said #Adobe big time we m...\n",
       "1952    Finally I got it .. thanx my father .. #Samsun...\n",
       "Name: tweet, Length: 1953, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet = test['tweet']\n",
    "test_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = test_pred\n",
    "\n",
    "submission = test[['id','label']]\n",
    "\n",
    "submission.to_csv('result_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
